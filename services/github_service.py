"""
GitHub Service - Repository Analysis and Optimization
Analyzes entire GitHub repositories for optimization opportunities
"""

import os
import asyncio
import requests
import base64
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
import tempfile
import subprocess

class GitHubService:
    def __init__(self):
        self.github_token = os.getenv("GITHUB_TOKEN")  # Optional for public repos
        
    async def analyze_repository(self, github_url: str) -> Dict[str, Any]:
        """Analyze an entire GitHub repository for optimization opportunities"""
        try:
            print(f"ðŸ” Analyzing GitHub repository: {github_url}")
            
            # Parse GitHub URL
            repo_info = self._parse_github_url(github_url)
            if not repo_info:
                return {"error": "Invalid GitHub URL"}
            
            # Fetch repository files
            print(f"ðŸ“ Fetching repository structure...")
            repo_files = await self._fetch_repository_files(repo_info)
            
            if not repo_files:
                return {"error": "Could not fetch repository files"}
            
            # Analyze code files
            print(f"ðŸ§  Analyzing {len(repo_files)} files with AI...")
            analysis_results = await self._analyze_codebase(repo_files, repo_info)
            
            # Generate optimization report
            optimization_report = await self._generate_optimization_report(analysis_results, repo_info)
            
            return {
                "repository": repo_info,
                "files_analyzed": len(repo_files),
                "analysis_results": analysis_results,
                "optimization_report": optimization_report,
                "github_integration": "complete"
            }
            
        except Exception as e:
            print(f"GitHub analysis error: {e}")
            return {"error": str(e)}
    
    def _parse_github_url(self, url: str) -> Optional[Dict[str, str]]:
        """Parse GitHub URL to extract owner and repo"""
        try:
            # Handle various GitHub URL formats
            if "github.com" not in url:
                return None
                
            # Remove .git suffix if present
            url = url.replace('.git', '')
            
            # Parse URL
            parsed = urlparse(url)
            path_parts = parsed.path.strip('/').split('/')
            
            if len(path_parts) >= 2:
                return {
                    "owner": path_parts[0],
                    "repo": path_parts[1],
                    "branch": "main",  # Default branch
                    "full_name": f"{path_parts[0]}/{path_parts[1]}"
                }
            return None
            
        except Exception as e:
            print(f"URL parsing error: {e}")
            return None
    
    async def _fetch_repository_files(self, repo_info: Dict[str, str]) -> List[Dict[str, Any]]:
        """Fetch all code files from the repository"""
        try:
            # GitHub API endpoints
            api_base = "https://api.github.com"
            headers = {}
            
            if self.github_token:
                headers["Authorization"] = f"token {self.github_token}"
            
            # Get repository tree
            tree_url = f"{api_base}/repos/{repo_info['full_name']}/git/trees/{repo_info['branch']}?recursive=1"
            response = requests.get(tree_url, headers=headers)
            
            if response.status_code != 200:
                print(f"GitHub API error: {response.status_code}")
                return []
            
            tree_data = response.json()
            code_files = []
            
            # Filter for code files
            code_extensions = {'.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.cs', '.go', '.rs', '.php', '.rb'}
            
            for item in tree_data.get('tree', []):
                if item['type'] == 'blob':  # It's a file
                    file_path = item['path']
                    file_ext = os.path.splitext(file_path)[1].lower()
                    
                    if file_ext in code_extensions:
                        # Fetch file content
                        file_content = await self._fetch_file_content(repo_info, file_path, headers)
                        
                        if file_content:
                            code_files.append({
                                "path": file_path,
                                "extension": file_ext,
                                "content": file_content,
                                "size": item.get('size', 0),
                                "language": self._detect_language(file_ext)
                            })
                        
                        # Limit to first 50 files for demo
                        if len(code_files) >= 50:
                            break
            
            return code_files
            
        except Exception as e:
            print(f"Error fetching repository files: {e}")
            return []
    
    async def _fetch_file_content(self, repo_info: Dict[str, str], file_path: str, headers: Dict[str, str]) -> Optional[str]:
        """Fetch content of a specific file"""
        try:
            api_base = "https://api.github.com"
            content_url = f"{api_base}/repos/{repo_info['full_name']}/contents/{file_path}"
            
            response = requests.get(content_url, headers=headers)
            
            if response.status_code == 200:
                content_data = response.json()
                
                if content_data.get('encoding') == 'base64':
                    # Decode base64 content
                    content = base64.b64decode(content_data['content']).decode('utf-8')
                    return content
            
            return None
            
        except Exception as e:
            print(f"Error fetching file {file_path}: {e}")
            return None
    
    def _detect_language(self, file_ext: str) -> str:
        """Detect programming language from file extension"""
        language_map = {
            '.py': 'python',
            '.js': 'javascript', 
            '.ts': 'typescript',
            '.jsx': 'javascript',
            '.tsx': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.cs': 'csharp',
            '.go': 'go',
            '.rs': 'rust',
            '.php': 'php',
            '.rb': 'ruby'
        }
        return language_map.get(file_ext, 'unknown')
    
    async def _analyze_codebase(self, code_files: List[Dict[str, Any]], repo_info: Dict[str, str]) -> Dict[str, Any]:
        """Analyze the entire codebase using our AI services"""
        try:
            # Import our services
            from services.captain_service import CaptainService
            from services.metorial_service import MetorialService
            
            captain = CaptainService()
            metorial = MetorialService()
            
            # Group files by language
            files_by_language = {}
            for file in code_files:
                lang = file['language']
                if lang not in files_by_language:
                    files_by_language[lang] = []
                files_by_language[lang].append(file)
            
            # Analyze each language group
            analysis_results = {
                "repository_overview": {
                    "total_files": len(code_files),
                    "languages_detected": list(files_by_language.keys()),
                    "largest_files": sorted(code_files, key=lambda x: x['size'], reverse=True)[:5]
                },
                "language_analysis": {},
                "optimization_opportunities": [],
                "algorithmic_patterns": [],
                "performance_hotspots": []
            }
            
            # Analyze each language
            for language, files in files_by_language.items():
                print(f"ðŸ” Analyzing {language} files...")
                
                # Combine code for analysis (sample first few files)
                combined_code = ""
                for file in files[:5]:  # Limit to first 5 files per language
                    combined_code += f"\n# File: {file['path']}\n{file['content']}\n"
                
                if combined_code.strip():
                    # Captain analysis
                    captain_analysis = await captain.analyze_code(
                        combined_code, 
                        language, 
                        "performance"
                    )
                    
                    # Metorial research
                    research = await metorial.research_optimizations(
                        language,
                        "performance", 
                        captain_analysis.get("patterns", [])
                    )
                    
                    # File-level analysis
                    file_analysis = []
                    for file in files:
                        file_insights = await self._analyze_single_file(file, captain)
                        file_analysis.append(file_insights)
                    
                    analysis_results["language_analysis"][language] = {
                        "files_count": len(files),
                        "captain_analysis": captain_analysis,
                        "research_findings": research,
                        "file_analysis": file_analysis,
                        "optimization_potential": self._calculate_optimization_potential(file_analysis)
                    }
            
            return analysis_results
            
        except Exception as e:
            print(f"Codebase analysis error: {e}")
            return {"error": str(e)}
    
    async def _analyze_single_file(self, file: Dict[str, Any], captain) -> Dict[str, Any]:
        """Analyze a single file for optimization opportunities"""
        try:
            # Quick analysis for each file
            file_content = file['content']
            
            # Detect algorithmic patterns
            algorithms_detected = self._detect_algorithms(file_content)
            
            # Performance analysis
            performance_issues = self._detect_performance_issues(file_content, file['language'])
            
            # Calculate complexity score
            complexity_score = self._calculate_complexity_score(file_content)
            
            return {
                "file_path": file['path'],
                "language": file['language'],
                "size": file['size'],
                "content": file_content,  # Include actual content for Captain analysis
                "algorithms_detected": algorithms_detected,
                "performance_issues": performance_issues,
                "complexity_score": complexity_score,
                "optimization_priority": "high" if complexity_score > 7 else "medium" if complexity_score > 3 else "low",
                "captain_ready": True  # Indicates this file is ready for Captain analysis
            }
            
        except Exception as e:
            return {
                "file_path": file['path'],
                "error": str(e)
            }
    
    def _detect_algorithms(self, code: str) -> List[Dict[str, Any]]:
        """Detect algorithmic patterns in code"""
        algorithms = []
        code_lower = code.lower()
        
        # Sorting algorithms
        if 'sort' in code_lower:
            if 'bubble' in code_lower or ('for' in code_lower and code_lower.count('for') >= 2):
                algorithms.append({
                    "type": "sorting",
                    "algorithm": "bubble_sort",
                    "optimization_potential": "high",
                    "suggestion": "Replace with quicksort or native sort function"
                })
            elif 'quick' in code_lower or 'partition' in code_lower:
                algorithms.append({
                    "type": "sorting", 
                    "algorithm": "quicksort",
                    "optimization_potential": "medium",
                    "suggestion": "Consider hybrid approach for small arrays"
                })
        
        # Search algorithms
        if any(term in code_lower for term in ['find', 'search', 'indexof']):
            if code_lower.count('for') > 0:
                algorithms.append({
                    "type": "search",
                    "algorithm": "linear_search", 
                    "optimization_potential": "high",
                    "suggestion": "Use binary search for sorted data or hash map for frequent lookups"
                })
        
        # Loop patterns
        nested_loops = code_lower.count('for') + code_lower.count('while')
        if nested_loops >= 2:
            algorithms.append({
                "type": "loops",
                "algorithm": "nested_loops",
                "optimization_potential": "high", 
                "suggestion": "Consider vectorization, caching, or algorithmic optimization"
            })
        
        # Recursion
        if 'def ' in code and any(func_name in code for func_name in ['fibonacci', 'factorial']):
            algorithms.append({
                "type": "recursion",
                "algorithm": "recursive_function",
                "optimization_potential": "high",
                "suggestion": "Add memoization or convert to iterative approach"
            })
        
        return algorithms
    
    def _detect_performance_issues(self, code: str, language: str) -> List[Dict[str, Any]]:
        """Detect performance issues in code"""
        issues = []
        code_lower = code.lower()
        
        # Language-specific issues
        if language == 'python':
            if '+=' in code and 'str' in code_lower:
                issues.append({
                    "issue": "string_concatenation",
                    "severity": "medium",
                    "description": "String concatenation in loop",
                    "suggestion": "Use join() or f-strings"
                })
                
            if 'list(' in code and 'range(' in code:
                issues.append({
                    "issue": "inefficient_list_creation", 
                    "severity": "low",
                    "description": "Inefficient list creation",
                    "suggestion": "Use list comprehension"
                })
        
        elif language == 'javascript':
            if 'document.getelementby' in code_lower:
                issues.append({
                    "issue": "dom_queries",
                    "severity": "medium", 
                    "description": "Repeated DOM queries",
                    "suggestion": "Cache DOM elements"
                })
                
            if code.count('var ') > 5:
                issues.append({
                    "issue": "var_usage",
                    "severity": "low",
                    "description": "Using var instead of let/const",
                    "suggestion": "Use let/const for better performance"
                })
        
        # General issues
        if code_lower.count('for') >= 3:
            issues.append({
                "issue": "deeply_nested_loops",
                "severity": "high",
                "description": "Deeply nested loops detected",
                "suggestion": "Consider algorithmic optimization"
            })
        
        return issues
    
    def _calculate_complexity_score(self, code: str) -> int:
        """Calculate complexity score for a file"""
        score = 0
        
        # Loop complexity
        score += code.lower().count('for') * 2
        score += code.lower().count('while') * 2
        
        # Conditional complexity  
        score += code.lower().count('if') 
        score += code.lower().count('elif')
        score += code.lower().count('else if')
        
        # Function complexity
        score += code.count('def ') + code.count('function ')
        
        # Nesting (approximation)
        max_indent = 0
        for line in code.split('\n'):
            indent = len(line) - len(line.lstrip())
            max_indent = max(max_indent, indent // 4)  # Assuming 4-space indents
        
        score += max_indent
        
        return min(score, 10)  # Cap at 10
    
    def _calculate_optimization_potential(self, file_analysis: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate overall optimization potential for a language"""
        if not file_analysis:
            return {"score": 0, "priority": "low"}
        
        high_priority = sum(1 for f in file_analysis if f.get("optimization_priority") == "high")
        medium_priority = sum(1 for f in file_analysis if f.get("optimization_priority") == "medium")
        
        total_files = len(file_analysis)
        score = (high_priority * 3 + medium_priority * 2) / total_files
        
        if score >= 2:
            priority = "high"
        elif score >= 1:
            priority = "medium" 
        else:
            priority = "low"
        
        return {
            "score": round(score, 2),
            "priority": priority,
            "high_priority_files": high_priority,
            "medium_priority_files": medium_priority,
            "total_files": total_files
        }
    
    async def _generate_optimization_report(self, analysis_results: Dict[str, Any], repo_info: Dict[str, str]) -> Dict[str, Any]:
        """Generate a comprehensive optimization report"""
        try:
            # Collect all optimization opportunities
            all_opportunities = []
            all_algorithms = []
            all_hotspots = []
            
            for language, lang_analysis in analysis_results.get("language_analysis", {}).items():
                for file_analysis in lang_analysis.get("file_analysis", []):
                    # Add algorithms found
                    for algo in file_analysis.get("algorithms_detected", []):
                        all_algorithms.append({
                            **algo,
                            "file": file_analysis["file_path"],
                            "language": language
                        })
                    
                    # Add performance issues
                    for issue in file_analysis.get("performance_issues", []):
                        all_opportunities.append({
                            **issue,
                            "file": file_analysis["file_path"],
                            "language": language
                        })
                    
                    # Add high complexity files as hotspots
                    if file_analysis.get("complexity_score", 0) > 6:
                        all_hotspots.append({
                            "file": file_analysis["file_path"],
                            "complexity": file_analysis["complexity_score"],
                            "language": language,
                            "priority": file_analysis.get("optimization_priority", "medium")
                        })
            
            # Generate summary
            report = {
                "repository_name": repo_info["full_name"],
                "analysis_summary": {
                    "total_files_analyzed": analysis_results["repository_overview"]["total_files"],
                    "languages_found": analysis_results["repository_overview"]["languages_detected"],
                    "algorithms_detected": len(all_algorithms),
                    "optimization_opportunities": len(all_opportunities),
                    "performance_hotspots": len(all_hotspots)
                },
                "top_algorithms": sorted(all_algorithms, key=lambda x: x.get("optimization_potential") == "high", reverse=True)[:10],
                "optimization_opportunities": sorted(all_opportunities, key=lambda x: x.get("severity") == "high", reverse=True)[:10],
                "performance_hotspots": sorted(all_hotspots, key=lambda x: x["complexity"], reverse=True)[:10],
                "recommendations": self._generate_recommendations(all_algorithms, all_opportunities, all_hotspots),
                "estimated_improvements": {
                    "performance_gain": "15-45% faster execution",
                    "memory_reduction": "10-30% less memory usage", 
                    "code_quality": "Significant maintainability improvement"
                }
            }
            
            return report
            
        except Exception as e:
            print(f"Error generating optimization report: {e}")
            return {"error": str(e)}
    
    def _generate_recommendations(self, algorithms: List[Dict], opportunities: List[Dict], hotspots: List[Dict]) -> List[Dict[str, Any]]:
        """Generate specific optimization recommendations"""
        recommendations = []
        
        # Algorithm optimizations
        bubble_sorts = [a for a in algorithms if a.get("algorithm") == "bubble_sort"]
        if bubble_sorts:
            recommendations.append({
                "type": "algorithmic",
                "priority": "high",
                "title": "Replace Bubble Sort Algorithms",
                "description": f"Found {len(bubble_sorts)} bubble sort implementations that can be optimized",
                "impact": "60-90% performance improvement",
                "files_affected": [a["file"] for a in bubble_sorts]
            })
        
        # Linear search optimizations
        linear_searches = [a for a in algorithms if a.get("algorithm") == "linear_search"]
        if linear_searches:
            recommendations.append({
                "type": "algorithmic",
                "priority": "high", 
                "title": "Optimize Search Operations",
                "description": f"Found {len(linear_searches)} linear search patterns",
                "impact": "50-80% faster search operations",
                "files_affected": [a["file"] for a in linear_searches]
            })
        
        # Loop optimizations
        nested_loops = [a for a in algorithms if a.get("algorithm") == "nested_loops"]
        if nested_loops:
            recommendations.append({
                "type": "performance",
                "priority": "medium",
                "title": "Optimize Nested Loops", 
                "description": f"Found {len(nested_loops)} nested loop patterns",
                "impact": "20-50% performance improvement",
                "files_affected": [a["file"] for a in nested_loops]
            })
        
        # High complexity files
        if hotspots:
            recommendations.append({
                "type": "refactoring",
                "priority": "medium",
                "title": "Simplify Complex Functions",
                "description": f"Found {len(hotspots)} files with high complexity",
                "impact": "Better maintainability and performance",
                "files_affected": [h["file"] for h in hotspots]
            })
        
        return recommendations[:10]  # Top 10 recommendations